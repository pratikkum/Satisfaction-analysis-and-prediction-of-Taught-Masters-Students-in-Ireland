---
title: "Satisfaction analysis and prediction of Taught Masters Students' in Ireland"
author: "Pratik (18250375)"
date: "August 4, 2019"
output:
  word_document: default
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries

Creating a function to get all the libraries into R Studio.

```{r Libraries}
run_lib <- function(){
  library(randomForest)
  library(e1071)
  library(rpart)
  library(leaps)
  library(pls)
  library(VGAM)
  library(MLmetrics)
  library(glue)
  library(glmnet)
  library(ggplot2) 
  library(ISLR)
  library(snakecase)
  library(GPArotation)
  library(psych)
  library(MASS)
  library(mvtnorm)
  library(dplyr)
  library(skimr) 
  library(caret) 
  library(tidyverse) 
  library(haven) 
  library(nnet) 
  library(Hmisc) 
  library(reshape2) 
  library(klaR)  
  library(ranger)
}
```

```{r, include=FALSE}
# Calling the Function to get all the libraries into R Studio
suppressWarnings(run_lib())
```

# Reading Data

Reading Raw Data in Sav Format.

```{r pressure, results='hide'}
eurostudent <- read_sav("E_V_Data.sav")
```

#  Filtering Dataset

Selecting relevant columns.

```{r}
euro_filtered <- subset(eurostudent[,c(3,4,6,7,14,15,16,17,18,19,20,21,22,23,24,
                                       25,57,65,66,67,77,87,97,110,112,135,136,137,
                                       218,219,220,223,243,244,245,246,247,248,249,
                                       252,253,258,259,260,261,262,263,272,273)])

# Step 1. Taught Masters Course and complete data for relevant columns.

taught_masters <- subset(euro_filtered,(Q1.1 == "6"))  
taught_masters <- taught_masters[,c(-1,-6:-10,-43:-47)]

# Skimming - To check missing values

Skimmed <- suppressWarnings(skim_to_wide(taught_masters))

# Step 2- Average monthly expenses missing

taught_masters <- subset(taught_masters,Q3.8.1c1 >= 0 & Q3.8.1c2 >= 0)

# Step 3. Students who haven't filled satisfaction rating missing

taught_masters <- subset(taught_masters,Q6.2.5 > 0 & Q6.2.1 > 0 & Q6.2.2 > 0)

# Step 4 - Q5.1 - What age are you (years)? missing

taught_masters <- subset(taught_masters,Q5.1 > 0) # 607 Observations

```

```{r, include=FALSE}

## Imputation 

# Step 1.
# Q1.12 - Did you enter college as a mature student? - Updating based on Age

for (i in 1:607){
  if (is.na(taught_masters$Q1.12[[i]]))
    taught_masters$Q1.12[i] <- 2
}

# Step 2.
# Updating NA values for below by mean of the columns : -

# Q1.14.1 - Quality of teaching
# Q1.14.2 - Organisation of studies and timetable
# Q1.14.3 - Possibility to select from a broad variety of courses
# Q1.14.4 - College administration's attitude toward students
# Q1.14.5 - Teaching staff's attitude towards students
# Q1.14.6 - Study facilities (e.g. library, computers, buildings, classrooms)

mean_temp <- round(mean(taught_masters$Q1.14.1, na.rm= TRUE))
for (i in 1:607){
  if (is.na(taught_masters$Q1.14.1[[i]]))
    taught_masters$Q1.14.1[i] <- mean_temp
}

mean_temp <- round(mean(taught_masters$Q1.14.2, na.rm= TRUE))
for (i in 1:607){
  if (is.na(taught_masters$Q1.14.2[[i]]))
    taught_masters$Q1.14.2[i] <- mean_temp
}

mean_temp <- round(mean(taught_masters$Q1.14.3, na.rm= TRUE))
for (i in 1:607){
  if (is.na(taught_masters$Q1.14.3[[i]]))
    taught_masters$Q1.14.3[i] <- mean_temp
}

mean_temp <- round(mean(taught_masters$Q1.14.4, na.rm= TRUE))
for (i in 1:607){
  if (is.na(taught_masters$Q1.14.4[[i]]))
    taught_masters$Q1.14.4[i] <- mean_temp
}

mean_temp <- round(mean(taught_masters$Q1.14.5, na.rm= TRUE))
for (i in 1:607){
  if (is.na(taught_masters$Q1.14.5[[i]]))
    taught_masters$Q1.14.5[i] <- mean_temp
}

mean_temp <- round(mean(taught_masters$Q1.14.6, na.rm= TRUE))
for (i in 1:607){
  if (is.na(taught_masters$Q1.14.6[[i]]))
    taught_masters$Q1.14.6[i] <- mean_temp
}
rm(mean_temp)

# Step 3. 
# Q3.3 - Do you pay a rent for your accommodation? - Updating by taking into account other related predictors (Only 3 Missing Values)

for (i in 1:607){
  if (is.na(taught_masters$Q3.3[[i]]))
    taught_masters$Q3.3[i] <- 2
}

# Step 4. 
# Q3.12 - Extent of financial difficulties? - Based on related financial predictors (3 - Neither) (Only 7 Missing values)

for (i in 1:607){
  if (is.na(taught_masters$Q3.12[[i]]))
    taught_masters$Q3.12[i] <- 3
}

# Step 5.
# Q3.21.1 - Time for study-related activities
# Q3.21.2 - Time for paid jobs (If applicable)
# Q3.21.3 - Total workload

# Update 3.21.2 to mean both Q3.21.1 and Q3.21.3, then
# Update 3.21.3 to mean both Q3.21.1 and Q3.21.2, then
# Update 3.21.1 to mean both Q3.21.2 and Q3.21.3, then
# Update each with column mean individually

for (i in 1:607){
  if (is.na(taught_masters$Q3.21.2[[i]]) &
      !is.na(taught_masters$Q3.21.1[[i]]) &
      !is.na(taught_masters$Q3.21.3[[i]]))
    taught_masters$Q3.21.2[i] <-
      round(mean(taught_masters$Q3.21.1[i],taught_masters$Q3.21.3[i], na.rm= TRUE))
}

for (i in 1:607){
  if (is.na(taught_masters$Q3.21.3[[i]]) &
      !is.na(taught_masters$Q3.21.1[[i]]) &
      !is.na(taught_masters$Q3.21.2[[i]]))
    taught_masters$Q3.21.3[i] <-
      round(mean(taught_masters$Q3.21.1[i],taught_masters$Q3.21.2[i], na.rm= TRUE))
}

for (i in 1:607){
  if (is.na(taught_masters$Q3.21.1[[i]]) &
      !is.na(taught_masters$Q3.21.3[[i]]) &
      !is.na(taught_masters$Q3.21.2[[i]]))
    taught_masters$Q3.21.1[i] <-
      round(mean(taught_masters$Q3.21.3[i],taught_masters$Q3.21.2[i], na.rm= TRUE))
}

for (i in 1:607){
  if (is.na(taught_masters$Q3.21.2[[i]]) &
      is.na(taught_masters$Q3.21.3[[i]]) &
      !is.na(taught_masters$Q3.21.1[[i]]))
        taught_masters$Q3.21.2[i] <-  taught_masters$Q3.21.1[i]
        taught_masters$Q3.21.3[i] <-  taught_masters$Q3.21.1[i]
}

for (i in 1:607){
  if (is.na(taught_masters$Q3.21.1[[i]]) &
      is.na(taught_masters$Q3.21.3[[i]]) &
      !is.na(taught_masters$Q3.21.2[[i]]))
    taught_masters$Q3.21.1[i] <-  taught_masters$Q3.21.2[i]
  taught_masters$Q3.21.3[i] <-  taught_masters$Q3.21.2[i]
}


mean_wl1 <-  round(mean(taught_masters$Q3.21.1, na.rm= TRUE))
for (i in 1:607){
  if (is.na(taught_masters$Q3.21.1[[i]]))
    taught_masters$Q3.21.1[i] <- mean_wl1
}
rm(mean_wl1)

mean_wl2 <- round(mean(taught_masters$Q3.21.2, na.rm= TRUE))
for (i in 1:607){
  if (is.na(taught_masters$Q3.21.2[[i]]))
    taught_masters$Q3.21.2[i] <- mean_wl2
}
rm(mean_wl2)

mean_wl3 <- round(mean(taught_masters$Q3.21.3, na.rm= TRUE))
for (i in 1:607){
  if (is.na(taught_masters$Q3.21.3[[i]]))
    taught_masters$Q3.21.3[i] <- mean_wl3
}
rm(mean_wl3)

# Step 6.
# Q3.5.2 - What is the most frequent mode ? Updating to 6 (Other) 

for (i in 1:607){
  if (is.na(taught_masters$Q3.5.2[[i]]))
    taught_masters$Q3.5.2[i] <- 6
}

# Step 7.
# Updating using means of the columns:-

# Q3.6.1 - On a typical day, time you cover from where you live to your HE 
# Q3.6.2 - On a typical day, distance you cover from where you live to your HE 

mean_temp <- round(mean(taught_masters$Q3.6.1, na.rm= TRUE))
for (i in 1:607){
  if (is.na(taught_masters$Q3.6.1[[i]]))
    taught_masters$Q3.6.1[i] <- mean_temp
}
rm(mean_temp)

mean_temp <- round(mean(taught_masters$Q3.6.2, na.rm= TRUE))
for (i in 1:607){
  if (is.na(taught_masters$Q3.6.2[[i]]))
    taught_masters$Q3.6.2[i] <- mean_temp
}
rm(mean_temp)

# Step 8.
# Updated as per survey requirements

# Q3.7 What is the average monthly disposable income (in Euro) ?

for (i in 1:607){
  if (is.na(taught_masters$Q3.7[i])) {
    taught_masters$Q3.7[i] <- 0
  }
}

# Step 9.
# Updated as per survey requirements

# Q3.14 - How many hours do you spend on paid jobs or paid
# internships in a typical week during a semester?

for (i in 1:607){
  if (is.na(taught_masters$Q3.14[[i]]))
    taught_masters$Q3.14[i] <- 0
}

# Step 10.
# Updated using neighbours/realted colums knowledge

# Q6.1.1 - I have felt cheerful and in good spirits  
# Q6.1.2 - I have felt calm and relaxed              
# Q6.1.3 - I have felt active and vigorous           
# Q6.1.4 - I have woken up feeling fresh and rested  

for (i in 1:607){
  if (is.na(taught_masters$Q6.1.2[[i]])){
    taught_masters$Q6.1.1[i] <- taught_masters$Q6.1.5[i]
    taught_masters$Q6.1.2[i] <- taught_masters$Q6.1.5[i]
    taught_masters$Q6.1.3[i] <- taught_masters$Q6.1.5[i]
    taught_masters$Q6.1.4[i] <- taught_masters$Q6.1.5[i]
  }
}

# Step 11
# Q5.4 - Would you consider yourself Irish: - Based on derived column

for (i in 1:607){
  if (is.na(taught_masters$Q5.4[[i]]))
    taught_masters$Q5.4[i] <- 5
}

# Step 12 
# Updating with mean of columns

# Q6.3 - How often do you drink alcohol?  - (Only 3 Missing value)
# Q6.7 - How frequently do you exercise? - (Only 5 Missing value)

mean_alc <- round(mean(taught_masters$Q6.3, na.rm= TRUE))  
for (i in 1:607){
  if (is.na(taught_masters$Q6.3[[i]]))
    taught_masters$Q6.3[i] <- mean_alc
}
rm(mean_alc)

mean_exc <- round(mean(taught_masters$Q6.7, na.rm= TRUE))  
for (i in 1:607){
  if (is.na(taught_masters$Q6.7[[i]]))
    taught_masters$Q6.7[i] <- mean_exc
}
rm(mean_exc)

# Step 13
# Updating with mean
# Q7.5 - Social standing 

mean_soc <- round(mean(taught_masters$Q7.5, na.rm= TRUE))  
for (i in 1:607){
  if (is.na(taught_masters$Q7.5[[i]]))
    taught_masters$Q7.5[i] <- mean_soc
}
rm(mean_soc)

# Q7.6 - Estimate ANNUAL income of your family household. (7 - Don't Know) - Updating  based on Survey requirement

for (i in 1:607){
  if (is.na(taught_masters$Q7.6[[i]]))
    taught_masters$Q7.6[i] <- 7
}

```

# Creating Backup 

Data is stored and read back in CSV format.

```{r}

write.csv(taught_masters, "taught_masters.csv", row.names=T)
taught_masters <- read.csv("taught_masters.csv", header=TRUE)
taught_masters$X = NULL;
dim(taught_masters)

# Final Dataset has 607 rows

taught_masters_n <- taught_masters

```

# Variable Conversion

Grouping Response Levels and Categorical variables for analysis and interpretation.

```{r}
# Re-grouped response to a scale of 1-3 as some of the groups had limited responses (unbalanced data). 

taught_masters_n$Q6.2.1 <- ifelse(taught_masters_n$Q6.2.1 < 3,1,
                                  ifelse((taught_masters_n$Q6.2.1 == 3), 2,
                                         ifelse(taught_masters_n$Q6.2.1 >= 4, 3,4)))

taught_masters_n$Q6.2.2 <- ifelse(taught_masters_n$Q6.2.2 < 3,1,
                                  ifelse((taught_masters_n$Q6.2.2 == 3 ), 2,
                                         ifelse(taught_masters_n$Q6.2.2 >= 4, 3,4)))
taught_masters_n$Q6.2.5 <- ifelse(taught_masters_n$Q6.2.5 < 3,1,
                                  ifelse((taught_masters_n$Q6.2.5 == 3 ), 2,
                                         ifelse(taught_masters_n$Q6.2.5 >= 4, 3,4)))

# Creating factors and groups for categorical predictors

taught_masters_n$Q5.4 <- ifelse(taught_masters_n$Q5.4 < 3,1,
                                ifelse(taught_masters_n$Q5.4 >= 3  , 2,3))

taught_masters_n$Q7.5 <- ifelse(taught_masters_n$Q7.5 < 5,1,
                                ifelse(taught_masters_n$Q7.5 >= 5  & taught_masters_n$Q7.5 < 8,2,
                                       ifelse(taught_masters_n$Q7.5 >= 8, 3,4)))

taught_masters_n$Q5.4     <- factor(taught_masters_n$Q5.4 )
taught_masters_n$Q7.5   <- factor(taught_masters_n$Q7.5 )
```
# Plots

Creating Plots for visualization.(1: - Unsatisfied; 2: - Neutral; 3: - Satisfied)

```{r , include=FALSE}
cols <- c("seagreen3","steelblue4", "slategray3")
```

## Gender

```{r, echo=FALSE}
tab2 <- table(taught_masters_n$Q6.2.5, taught_masters_n$Q5.2) # Gender
prop.table(tab2,2)
barplot(prop.table(tab2,2), beside=TRUE, col=cols,
        legend.text=TRUE, args.legend= (list(x=5.5,y=1.1)),xlab = "1:FEMALE              2:MALE")
```

## Nationality

```{r, echo=FALSE}
tab2<- table(taught_masters_n$Q6.2.5,taught_masters_n$Q5.4) 
prop.table(tab2,2)

barplot(prop.table(tab2,2), beside=TRUE, col=cols,
        legend.text=TRUE, args.legend= (list(x=5.5,y=1)),xlab ="1:Irish                2:Non-Irish" )
```


## Social Standing

```{r, echo=FALSE}
tab2<- table(taught_masters_n$Q6.2.5,taught_masters_n$Q7.5) 
prop.table(tab2,2)

barplot(prop.table(tab2,2), col=cols,
        legend.text=TRUE, args.legend= (list(x=2.5,y=1.35)),xlab ="1:Low     2:Medium    3:High" )
```

## Mature Student

```{r, echo=FALSE}

tab2<- table(taught_masters_n$Q6.2.5,taught_masters_n$Q1.12) 
prop.table(tab2,2)

barplot(prop.table(tab2,2), beside=TRUE, col=cols,
        legend.text=TRUE, args.legend= (list(x=5.8,y=1.1)),xlab= "1:Mature                2:Not-mature  ")
```

#  Splitting Data

Data is partitioned in 2:1 ratio with training and testing set.

```{r, results="hide"}

trainIndex1 <- createDataPartition(taught_masters_n$Q6.2.5, p = .67,
                                   list = FALSE,times = 1)
Train1_n <- taught_masters_n[trainIndex1,]
Test1_n <- taught_masters_n[-trainIndex1,]

trainIndex2 <- createDataPartition(taught_masters_n$Q6.2.1, p = .67,
                                   list = FALSE,times = 1)
Train2_n <- taught_masters_n[trainIndex2,]
Test2_n <- taught_masters_n[-trainIndex2,]

trainIndex3 <- createDataPartition(taught_masters_n$Q6.2.2, p = .67,
                                   list = FALSE,times = 1)
Train3_n <- taught_masters_n[trainIndex3,]
Test3_n <- taught_masters_n[-trainIndex3,]

rm(trainIndex1,trainIndex2,trainIndex3)
```

# Data Modelling

## 1. Satisfaction with College (Reponse)

### Linear Models

In this thesis, I first check the associations with the utilization of classical regression analysis. This methodology gives us an initial understanding of dependent and independent factors. But in our case the data is not normally distributed and has a non-linear shape, thus findings of the analysis are indecisive and not used in results formulation and model comparison at a later stage. 

```{r, echo=FALSE}

fit_satisf_lm <- lm (Q6.2.5 ~ Q1.4+Q1.14.1+Q1.14.2+Q1.14.3+Q1.14.4+Q1.14.5+
                      Q1.14.6+Q1.12+Q5.2+Q5.4+Q7.5,
                    data=taught_masters_n)

# Removing insignificant Predictors

fit_satisf_lm <- lm (Q6.2.5 ~ Q1.14.1+Q1.14.2+Q1.14.5+Q1.14.6, data=Train1_n)

summary(fit_satisf_lm)

# Now all the predictors are significant.

```

The response is "Students perceived satisfaction with college" and significant predictors are derived , where P-Value<0.05, i.e. in our case Satisfaction with Organisation of timetable and studies, Teaching Quality, Teaching staff's attitude as well as Study amenities (e.g. computers, library, classrooms). For all the significant predictors , coefficients are positive, i.e. Satisfaction with college increases with an increase in satisfaction with the predictors. 

###  SVM

```{r, echo=FALSE}

set.seed(100)
fit_svm <- svm (factor (Q6.2.5) ~ Q1.14.1+Q1.14.2+Q1.14.4+Q1.14.5+Q1.14.6, 
               data = Train1_n)
summary(fit_svm)

Pred_svm <- predict (fit_svm, Test1_n)

confusionMatrix(as.factor(Test1_n$Q6.2.5),Pred_svm)
F1_Score (Test1_n$Q6.2.5, Pred_svm) 
```

Multi-category SVM algorithm from the e1071 package (7) is used for analysis. SVM fits the test data correctly and misclassified only 22.5% observations. Again, in this case, Group 2 ("Neutral" in terms of satisfaction with the college) is misclassified 100%. Overall accuracy is 77.5% and it does a great job in predicting "Unsatisfied" and "Satisfied" students. 

###  Random Forest

"randomForest" function inside "randomForest" package is used for analysis.

```{r, echo=FALSE}

fit_randomf <- randomForest(factor(Q6.2.5) ~ Q1.14.1+Q1.14.2+Q1.14.4+Q1.14.5+
                              Q1.14.6, data = Train1_n, mtry=3)

Pred_randomf <- predict (fit_randomf, Test1_n)

confusionMatrix(as.factor(Test1_n$Q6.2.5), Pred_randomf)
F1_Score (Test1_n$Q6.2.5, Pred_randomf)

```

Random Forest fits the test data correctly and misclassified only 19% observations. 
Group 2 ("Neutral" in terms of satisfaction with the college) is misclassified 100%. Overall accuracy is 81% and it does a great job in predicting "Unsatisfied" and "Satisfied" students.

```{r,echo=FALSE}
varImpPlot (fit_randomf, pch = 20, main = "Importance of Variables")
```

"Importance ranking" for predictors is also shown in the above plot.

###  Linear Discriminant Analysis

LDA assumes that the predictors are normally distributed. As stated earlier that we have data that is not normally distributed, but we are still interested in the prediction accuracy, hence we implement LDA.

```{r, echo=FALSE}

fit_satisf_lda <- lda (factor (Q6.2.5) ~ Q1.14.1+Q1.14.2+Q1.14.4+Q1.14.5+Q1.14.6,
                       data=Train1_n)
fit_satisf_lda

Pred_lda <- predict (fit_satisf_lda, Test1_n)$class

F1_Score (Test1_n$Q6.2.5, Pred_lda, positive = NULL) 

```

There are 451 observations, 43 in class 1, 42 in class 2 and 366 in class 3. LD1 is the first linear discriminant which is a linear combination of below parameters: -

(Q1.14.1*0.3611706) + (Q1.14.2*0.3136274) + (Q1.14.4*0.1703964) + (Q1.14.5*0.5724015) + (Q1.14.6*0.2461356) 

Teaching staff's behavior towards students, Teaching Quality and Organisation of timetable and studies have the largest coefficients which signify them as the most important parameters in the estimation of response.
All the coefficients are positive, this resonates the result that increases in satisfaction of any of the included predictors in the model lead to an overall increase in student satisfaction with college.
LD1 explains 97.54% of the overall variance.

```{r, echo=FALSE}
partimat (factor (Q6.2.5) ~ Q1.14.1+Q1.14.2+Q1.14.4+Q1.14.5+Q1.14.6, 
         data=Test1_n, method="lda") # klaR
```

Partition plots are a series of plots for all combinations of categories. The plot shows the three groups with overlapping boundary. It shows that Group 1 and 3 are fairly distinguishable, but Group 2 has major overlapping with the boundary of both the remaining groups and hence it is difficult to correctly predict if a student is "Neutral" in terms of satisfaction with the college.

```{r,echo=FALSE}
confusionMatrix( as.factor (Test1_n$Q6.2.5), Pred_lda)
```

This model fit the test data correctly and misclassified only 17% observations. But it doesn't predict group 2 correctly as there is a lot overlap between the adjacent classes hence it is very difficult to predict if a student is "Neutral" in terms of satisfaction with the college. Otherwise, it does a great job of predicting "Unsatisfied" or "Satisfied" students accurately. The model performs comprehensively well with the test data (not used for modeling) having an overall classification accuracy of 83%.


###  Quadratic Discriminant Analysis

```{r, echo=FALSE}

fit_satisf_qda <- qda (factor (Q6.2.5) ~ Q1.14.1+Q1.14.2+Q1.14.4+Q1.14.5+Q1.14.6 ,
                      data=Train1_n)
summary(fit_satisf_qda)

Pred_qda <- predict (fit_satisf_qda, Test1_n)$class
partimat (factor (Q6.2.5) ~ Q1.14.1+Q1.14.2+Q1.14.4+Q1.14.5+Q1.14.6, 
         data=Test1_n, method="qda") # klaR

```

Group 1 and 3 are fairly distinguishable, but Group 2 has an overlapping boundary with remaining groups and hence it is difficult to correctly predict if a student is "Neutral" in terms of satisfaction with the college.

```{r,echo=FALSE}
confusionMatrix (as.factor (Test1_n$Q6.2.5), Pred_qda)
F1_Score (Test1_n$Q6.2.5, Pred_qda, positive = NULL) 
```

There are 451 observations, 43 in class 1, 42 in class 2 and 366 in class 3. QDA model predicts the test data correctly but misclassified slightly more that LDA with 22.7% observations. Again Group 2 ("Neutral" in terms of satisfaction with the college) is incorrectly predicted 100% times. Overall it does a fair job in predicting "Unsatisfied" or "Satisfied" students accurately with an accuracy of 77.3%, but not as good as LDA.


###  Multinomial Logistic Regression

Now once we have established the final set of predictors using the above chapter, we can analyze their relationship with the response. 

```{r, echo=FALSE}

fil_multi <- multinom(factor(Q6.2.5) ~ Q1.14.1+Q1.14.2+Q1.14.4+Q1.14.5+Q1.14.6, 
                      data = Train1_n)
summary(fil_multi)
exp(coef(fil_multi))

Pred_mnr <- predict (fil_multi, Test1_n)

confusionMatrix(as.factor(Test1_n$Q6.2.5), Pred_mnr)
F1_Score (Test1_n$Q6.2.5, Pred_mnr, positive = NULL) # 0.51

pnorm(abs(summary(fil_multi)$coefficients / summary(fil_multi)$standard.errors),
      lower.tail=FALSE)*2
```

Neutral vs Unsatisfied Students - 

With P-values (< 0.05) only Q1.14.1 is significant.

For a one-level increase in satisfaction for Quality of teaching, we expect to see an 87% (OR = 1.87) rise in the odds of being Neutral vs Unsatisfied in terms of satisfaction from college.

Satisfied vs Unsatisfied Students -

With P-values (< 0.05) all predictors Q1.14.1, Q1.14.2, Q1.14.4, Q1.14.5, Q1.14.6 are significant. 
For example, we expect to see 113% (OR = 2.13) increase in the odds of being Satisfied vs Unsatisfied for one level increase in satisfaction of Quality of teaching. Similarly, odds of being Satisfied vs Unsatisfied for one level increase in satisfaction of Q1.14.2, Q1.14.4, Q1.14.5, Q1.14.6 increases by 93%, 54%, 97% and 91% (OR = 1.937122 / 1.549803 / 1.971980 / 1.915410). 

###  Ordinal regression

After having considered the nominal approach, we will now discuss the ordinal approach that helps to answer another interesting question that how the satisfaction responses vary when ranked from 1-3.

```{r, echo=FALSE}

fit_ordi <- polr(factor(Q6.2.5) ~ Q1.14.1+Q1.14.2+Q1.14.4+Q1.14.5+Q1.14.6,
                 data = Train1_n, Hess=TRUE)
summary(fit_ordi)
exp(coef(fit_ordi))

Pred_or <- predict(fit_ordi, Test1_n)

confusionMatrix( as.factor(Test1_n$Q6.2.5),Pred_or)
F1_Score(Test1_n$Q6.2.5,Pred_or)           # 0.5

(ctable <- coef(summary(fit_ordi)))
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
cbind(ctable, "p value" = p)

```
With P-values (< 0.05) all predictors Q1.14.1, Q1.14.2, Q1.14.4, Q1.14.5, Q1.14.6 are significant.

We expect to see 61% (OR = 1.61) increase in the odds of moving from Unsatisfied to Neutral or Satisfied for one level increase in satisfaction of Quality of teaching (Q1.14.1). Due to the property of proportional odds assumption, the same increase, 1.61 times, is found between Satisfied vs Neutral or Unsatisfied.

Similarly, odds of being Unsatisfied vs Neutral or Satisfied for one level increase in satisfaction of Q1.14.2, Q1.14.4, Q1.14.5, Q1.14.6 increases by 57%, 38%, 89.9% and 58.7% (OR =1.57 / 1.37 / 1.899 / 1.587). Similar conclusion for Satisfied vs Neutral or Unsatisfied category due to proportional odds assumption.


## 2. Satisfaction with Accommodation (Response)

### Linear Models

In this thesis, I first check the associations with the utilization of classical regression analysis. This methodology gives us an initial understanding of dependent and independent factors. But in our case the data is not normally distributed and has a non-linear shape, thus findings of the analysis are indecisive and not used in results formulation and model comparison at a later stage. 

```{r, echo=FALSE}
fit_satisf_lm2 <- lm(Q6.2.1 ~ Q3.8.1c1+Q6.1.1+Q3.12+Q5.2+Q3.6.2+
                       Q3.3+Q1.12+Q3.5.2+Q5.1 ,data=Train2_n)
fit_satisf_lm2 <- lm(Q6.2.1 ~ Q3.8.1c1+Q3.12+Q5.2+Q6.1.1,data=Train2_n)

summary(fit_satisf_lm2)

# Now all the predictors are significant.

```

The response is students perceived satisfaction with accommodation which is derived from predictors (P-Value<0.05).

Gender [1- Female and 2- Male]  - Coefficient is negative which signifies that satisfaction level is less for a male when compared to female students.

Perceived level of financial difficulties - Negative coefficient signifies that satisfaction level decreases when students are facing more financial difficulty.

Feeling of being in good spirits and cheerful over last two weeks - Positive coefficient signifies that satisfaction level increases when students have felt more cheerful on a scale of 1-6 (At no Time - All of the Time).

Average monthly expenditure (Own Pocket) - Positive coefficient signifies that satisfaction level increases when student's expenditure increases. This is a bit out of the box and we need to re-validate this outcome when we derive final conclusions after comparing other models.

### SVM

```{r, echo=FALSE}

fit_svm2 <- svm(factor(Q6.2.1) ~  Q3.3+Q3.12+Q3.8.1c1+Q6.1.1+Q5.2,data = Train2_n)

Pred_svm2 <- predict(fit_svm2, Test2_n)
confusionMatrix( as.factor(Test2_n$Q6.2.1),Pred_svm2)
F1_Score(Test2_n$Q6.2.1,Pred_svm2)

```

Multi-category SVM algorithm from the e1071 package is used for analysis. SVM fits the test data correctly and misclassified only 20% observations. In this case, Group 1 and 2 are misclassified 100% of the times. Hence it is not a good model for satisfaction prediction with accommodation for desired reasons.

###  Random Forest 

```{r, echo=FALSE}
fit_randomf2 <- randomForest(factor(Q6.2.1) ~ Q3.3+Q3.12+
                               Q3.8.1c1+Q6.1.1+Q5.2,
                             data = Train2_n)

Pred_randomf2 <- predict(fit_randomf2, Test2_n)

confusionMatrix( as.factor(Test2_n$Q6.2.1),Pred_randomf2)
F1_Score(Test2_n$Q6.2.1,Pred_randomf2) 
```

Random Forest fits the test data correctly and misclassified only 21% observations. In this case, Group 2 is misclassified 100% of the times. Overall accuracy is 79% and it does a great job in predicting "Unsatisfied" and "Satisfied" student categories for satisfaction with the accommodation.

```{r,echo=FALSE}
varImpPlot(fit_randomf2,sort = T,n.var=7, pch = 20, main = "Importance of Variables")
```

"Importance ranking" for predictors is also shown in the above plot.

###  Linear Discriminant Analysis

LDA assumes that the predictors are normally distributed. As stated earlier that we have data that is not normally distributed, but we are still interested in the prediction accuracy, hence we implement LDA.

```{r, echo=FALSE}
fit_satisf_lda2 <- lda(factor(Q6.2.1) ~ Q3.3+Q3.12+
                          Q3.8.1c1+Q6.1.1+Q5.2 ,
                        data=Train2_n)
fit_satisf_lda2
table(Train2_n$Q6.2.1)

Pred_lda2 <- predict(fit_satisf_lda2, Test2_n)$class

```

There are 407 total observations, 45 in class 1, 34 in class 2 and 328 in class 3. 
LD1 is the first linear discriminant which is a linear combination of below parameters: -

(Q3.32* 0.4182167604 + Q3.12*-0.4737921525 + Q3.8.1c1* 0.0005513997 + Q6.1.1* 0.3628719222 + Q5.22*-1.0241931415)

Q5.22, Q3.32, and Q3.12 have the largest coefficients, hence these are the most significant predictors in influencing the response.

Q3.12 and Q5.22 have negative coefficients and the rest of the parameters have positive coefficients, this resonates the result from linear regression for predicting student satisfaction with the accommodation. LD1 explains 94.72% of the overall variance in the data.


```{r, echo=FALSE}
partimat(factor(Q6.2.1) ~ Q3.3+Q1.2+Q3.12+Q5.2,
         data=Test2_n, method="lda") # klaR
```

In this case, all the 3 Groups have overlapping boundaries and hence the model predicts all the students to be in group 3 "Satisfied" with the accommodation. It is eventually difficult to separate the classes and predict unless we have balanced data.

```{r,echo=FALSE}
confusionMatrix( as.factor(Test2_n$Q6.2.1),Pred_lda2)
F1_Score(Test2_n$Q6.2.1,Pred_lda2, positive = NULL)  
```

This model fit the test data correctly and misclassified only 20% observations. But it doesn't predict group 2 correctly 100% of times as there is a complete overlap between the adjacent classes, hence it is very difficult to predict if a student is "Neutral" in terms of satisfaction with the accommodation. The model accuracy is 80% on the test data.

###  Quadratic Discriminant Analysis

```{r, echo=FALSE}
fit_satisf_qda2 <- qda(factor(Q6.2.1) ~ Q3.3+Q3.12+
                         Q3.8.1c1+Q6.1.1+Q5.2,data=Train2_n)
summary(fit_satisf_qda2)
fit_satisf_qda2

Pred_qda2 <- predict(fit_satisf_qda2, Test2_n)$class
partimat(factor(Q6.2.1) ~ Q3.3+Q3.12+
           Q3.8.1c1+Q6.1.1+Q5.2,
         data=Test2_n, method="qda") # klaR

```

Again, the model predicts most of the students to be in group 3 "Satisfied" with the accommodation with indistinguishable boundaries with its neighbors.

```{r,echo=FALSE}
confusionMatrix( as.factor(Test2_n$Q6.2.1),Pred_qda2)
F1_Score(Test2_n$Q6.2.1,Pred_qda2, positive = NULL) 
```

There are 407 observations, 43 in class 1, 33 in class 2 and 331 in class 3. QDA model assists in predicting the test data correctly but misclassified slightly more that LDA with 23% observations. Again Group 2 ("Neutral" in terms of satisfaction with the college) is incorrectly predicted 100% times. Overall it predicts student satisfaction accommodation with an accuracy of 77%.

###  Multinomial Logistic Regression

Now once we have established the final set of predictors using the above chapter, we can analyze their relationship with the response. 

```{r, echo=FALSE}
fil_multi2 <- multinom(factor(Q6.2.1) ~ Q3.3+Q3.12+
                         Q3.8.1c1+Q6.1.1+Q5.2, 
                       data = Train2_n)
summary(fil_multi2)

Pred_mnr2 <- predict(fil_multi2, Test2_n)

confusionMatrix( as.factor(Test2_n$Q6.2.1),Pred_mnr2)
F1_Score(Test2_n$Q6.2.1,Pred_mnr2, positive = NULL) 

pnorm(abs(summary(fil_multi2)$coefficients / summary(fil_multi2)$standard.errors),
      lower.tail=FALSE)*2
exp(coef(fil_multi2))

```

Neutral vs Unsatisfied Students

With P-values (< 0.05) only Q3.12 is significant. For a one-level increase in perceived financial difficulties on a scale 1-5 (No Difficulty to Serious Difficulties), we expect to see a 53% (OR = 0.53) decrease in the odds of being Neutral vs Unsatisfied in terms of satisfaction with the accommodation. There exists an inverse relationship between satisfaction with accommodation and level of financial difficulty, which seems logical.

Satisfied vs Unsatisfied Students
With P-values (< 0.05) predictors Q5.22, Q6.1.1, Q3.12, and Q3.8.1c1 are significant. For understanding the relationships, we need to analyze the odds ratio for all the predictors as below: -

Q3.12- We expect to see 53% (OR = 0.53) decrease in the odds of being Satisfied vs Unsatisfied for one level increase in perceived financial difficulties on a scale of 1-5.

Q5.22 - For Male vs Female, we expect to see 37% (OR = 0.53) decrease in the odds of being Satisfied vs Unsatisfied with accommodation. Female students are more satisfied with their accommodation comparatively.

Q6.1.1 - We expect to see 43% (OR = 1.43) increase in the odds of being Satisfied vs Unsatisfied for one level increase in perceived feeling more cheerful on a scale of 1-6 (At no Time - All of the Time). Hence, cheerful students are almost twice as more satisfied with their accommodation compared to other students.

Q3.8.1c1 - With the increase in average monthly expenditure (Own Pocket in Euros) on a nominal scale, we expect to see 0.1% (OR = 1.001) increase in the odds of being Satisfied vs Unsatisfied with accommodation. This may be due to the fact that students with higher expenditure may be paying higher rents as well for better houses, in turn leading to more satisfaction with the accommodation.

###  Ordinal regression

After having considered the nominal approach, we will now discuss the ordinal approach that helps to answer another interesting question that how the satisfaction responses vary when ranked from 1-3.

```{r, echo=FALSE}

fit_ordi2 <- polr(factor(Q6.2.1) ~ Q3.3+Q3.12+
                    Q3.8.1c1+Q6.1.1+Q5.2,
                  data = Train2_n, Hess=TRUE)
summary(fit_ordi2)


Pred_or2 <- predict(fit_ordi2, Test2_n)

confusionMatrix( as.factor(Test2_n$Q6.2.1),Pred_or2)
F1_Score(Test2_n$Q6.2.1,Pred_or2)           

(ctable2 <- coef(summary(fit_ordi2)))
p2 <- pnorm(abs(ctable2[, "t value"]), lower.tail = FALSE) * 2
cbind(ctable2, "p value" = p2)
exp(coef(fit_ordi2))

```

With P-values (< 0.05) predictors Q5.22, Q6.1.1, Q3.12 and Q3.8.1c1 are significant. [Figure 33].
We expect to see 70% (OR = 0.70) decrease in the odds of moving from Unsatisfied to Neutral or Satisfied for one level increase in perceived financial difficulties (Q3.12) on a scale of 1-5. Due to the property of proportional odds assumption, the same decrease, 0.7 times, is deduced between Satisfied vs Neutral or Unsatisfied.
Similarly, odds of being Unsatisfied vs Neutral or Satisfied for other predictors can be analyzed as below: -

Q5.22 - For Male vs Female, we expect to see 46% (OR = 0.46) decrease. Again, this suggests that female counterparts are comparatively more satisfied with their accommodation.

Q6.1.1 - We expect to see 37% (OR = 1.37) increase for one level increase in perceived feeling more cheerful on a scale of 1-6 (At no Time - All of the Time). Hence, students who haven't felt in good spirits (over the last two weeks) are less satisfied with their accommodation.

Q3.8.1c1 - With an increase in average monthly expenditure (Own Pocket in Euros) on a nominal scale, we expect to see a 0.1% (OR = 1.001) increase.

Because of the proportional odds assumption, the similar results apply for predicting Satisfied vs Neutral or Unsatisfied students.

## 3. Satisfaction with Financial/Material wellbeing (Response)

### Linear Models

In this thesis, I first check the associations with the utilization of classical regression analysis. This methodology gives us an initial understanding of dependent and independent factors. But in our case the data is not normally distributed and has a non-linear shape, thus findings of the analysis are indecisive and not used in results formulation and model comparison at a later stage. 

```{r, echo=FALSE}

fit_satisf_lm4 <- lm(Q6.2.2 ~ Q3.12+Q6.1.1+Q5.1+Q5.4+
                       Q3.7+Q3.8.1c1+Q5.2+Q6.1.2, data=Train3_n)
fit_satisf_lm4 <- lm(Q6.2.2 ~ Q3.12+Q6.1.1 ,data=Train3_n)

summary(fit_satisf_lm4)

# Now both the predictors are significant. 

```
Students perceived satisfaction with financial and material wellbeing is derived from predictors (P-Value<0.05).

Perceived level of financial difficulties - Negative coefficient sign suggests that satisfaction level decreases for students facing higher levels of financial difficulty.

Feeling of being in good spirits and cheerful (over last two weeks) - Positive coefficient suggests that satisfaction level rises for students have felt more cheerful over last two weeks on a scale of 1-6 (At no Time - All of the Time).


### SVM

```{r, echo=FALSE}

fit_svm3 <- svm(factor(Q6.2.2) ~  Q3.12+Q6.1.1 , 
                data = Train3_n)
summary(fit_svm3)

Pred_svm3 <- predict(fit_svm3, Test3_n)

confusionMatrix(factor(Test3_n$Q6.2.2),Pred_svm3)
F1_Score(Test3_n$Q6.2.2,Pred_svm3) 

```

Multi-category SVM algorithm from the e1071 package (7) is used for analysis. SVM fits the test data correctly and misclassified 26% observations. Again, in this case, Group 2 ("Neutral" in terms of satisfaction with wellbeing) is misclassified 100% of the times. Overall accuracy is 74% and it does a reasonable job in predicting "Unsatisfied" and "Satisfied" students.

### Random Forest 

```{r, echo=FALSE}
fit_randomf3 <- randomForest(factor(Q6.2.2) ~ Q3.12+Q6.1.1+Q5.1+
                Q3.6.2+Q3.8.1c1,data = Train3_n,ntree=350)

Pred_randomf3 <- predict(fit_randomf3, Test3_n)

confusionMatrix(factor(Test3_n$Q6.2.2),Pred_randomf3)
F1_Score(Test3_n$Q6.2.2,Pred_randomf3) 

```

Random Forest fits the test data correctly and misclassified more than a quarter (27%) of observations. This time it does not completely misclassify all observations in Group 2 ("Neutral" satisfaction. Overall accuracy is 73% and it does a fair job in predicting all classes of students for satisfaction with wellbeing.

```{r,echo=FALSE}

varImpPlot(fit_randomf3,sort = T,n.var=8, pch = 20, 
           main = "Importance of Variables")
```

"Importance ranking" for predictors is also shown in the above plot.

### Linear Discriminant Analysis

LDA assumes that the predictors are normally distributed. As stated earlier that we have data that is not normally distributed, but we are still interested in the prediction accuracy, hence we implement LDA.

```{r, echo=FALSE}
fit_satisf_lda3 <- lda( factor(Q6.2.2) ~ Q3.12+Q6.1.1,
                        data=Train3_n)
fit_satisf_lda3
table(Train3_n$Q6.2.2)

Pred_lda3 <- predict(fit_satisf_lda3, Test3_n)$class
```

There are 407 total observations, 181 in class 1, 73 in class 2 and 153 in class 3.

LD1 is the first linear discriminant which is a linear combination of below parameters: -

(Q3.12*-1.0395162 + Q6.1.1* 0.2145408)

Q3.12 has negative coefficient and Q6.1.1 has a positive coefficient, this is similar to the results obtained from linear regression. LD1 explains almost all (99.58%) of the variability in the data.


```{r, echo=FALSE}
partimat(factor(Q6.2.2) ~ Q3.12+Q6.1.1, data=Test3_n, method="lda") # klaR
```

Group 1 and 3 are fairly distinguishable, but Group 2 has complete overlapping with neighboring boundaries and hence it is difficult to correctly predict if a student is "Neutral" in terms of satisfaction with the wellbeing.

```{r,echo=FALSE}
confusionMatrix(factor(Test3_n$Q6.2.2),Pred_lda3)
F1_Score(Test3_n$Q6.2.2,Pred_lda3, positive = NULL)
```

This model fit the test data correctly and misclassified 26% observations. But it doesn't predict group 2 correctly 100% of times, as there is a complete overlap between the adjacent classes. The model has reasonably well accuracy on the test data of 74% for predicting of student satisfaction with the wellbeing. 

### Quadratic Discriminant Analysis

```{r, echo=FALSE}
fit_satisf_qda3 <- qda(factor(Q6.2.2) ~ Q3.12+Q6.1.1,
                       data=Train3_n)
fit_satisf_qda3
table(Train3_n$Q6.2.2)

Pred_qda3 <- predict(fit_satisf_qda3, Test3_n)$class
partimat(factor(Q6.2.2) ~ Q3.12+Q6.1.1,
         data=Test3_n, method="qda") # klaR

```
Group 1 "Unsatisfied" or Group 3 "Satisfied", but Group 2 "Neutral" has an overlapping boundary with remaining groups and hence it is difficult to correctly predict if a student is "Neutral" in terms of satisfaction with the wellbeing. 

```{r,echo=FALSE}
confusionMatrix(factor(Test3_n$Q6.2.2),Pred_qda3)
F1_Score(Test3_n$Q6.2.2,Pred_qda3, positive = NULL) 
```

There are 451 observations, 43 in class 1, 42 in class 2 and 366 in class 3.QDA performs same as LDA, in this case, misclassified 26% observations. Again Group 2 ("Neutral" in terms of satisfaction with the college) is incorrectly predicted 100% times. Overall it predicts student satisfaction with wellbeing with an accuracy of 74%.

### Multinomial Logistic Regression

Now once we have established the final set of predictors using the above chapter, we can analyze their relationship with the response. 

```{r, echo=FALSE}
fil_multi3 <- multinom(factor(Q6.2.2) ~ Q3.12+Q6.1.1, 
                       data = Train3_n)
summary(fil_multi3)

Pred_mnr3 <- predict(fil_multi3, Test3_n)

confusionMatrix(factor(Test3_n$Q6.2.2),Pred_mnr3)
F1_Score(Test3_n$Q6.2.2,Pred_mnr3, positive = NULL)

pnorm(abs(summary(fil_multi3)$coefficients / 
            summary(fil_multi3)$standard.errors),
      lower.tail=FALSE)*2
exp(coef(fil_multi3))

```

Neutral vs Unsatisfied Students - With P-values (< 0.05) significant parameters are.

Q3.12 - For a one-level increase in perceived financial difficulties on a scale 1-5 (No - Serious Difficulties), we expect to see a 32% (OR = 0.32) decrease in the odds in terms of satisfaction with wellbeing.

Q6.1.1 - We expect to see 48% (OR = 1.48) increase in the odds for one level increase in perceived feeling more cheerful on a scale of 1-6 (At no Time - All of the Time). 

Satisfied vs Unsatisfied Students - With P-values (< 0.05) significant parameters are.

Q3.12- We expect to see 14% (OR = 0.14) decrease in the odds for one level increase in perceived financial difficulties on a scale of 1-5. There exists an inverse relationship with financial difficulty and satisfaction with wellbeing.

Q6.1.1 - We expect to see 53% (OR = 1.53) increase in the odds for one level increase in perceived feeling more cheerful on a scale of 1-6 (At no Time - All of the Time). Hence, satisfaction with accommodation increases if the student has felt more cheerful over the last couple of weeks.


### Ordinal regression

After having considered the nominal approach, we will now discuss the ordinal approach that helps to answer another interesting question that how the satisfaction responses vary when ranked from 1-3.

```{r, echo=FALSE}
fit_ordi3 <- polr(factor(Q6.2.2) ~  Q3.12+Q6.1.1,
                  data = Train3_n, Hess=TRUE)
summary(fit_ordi3)

Pred_or3 <- predict(fit_ordi3, Test3_n)

confusionMatrix(factor(Test3_n$Q6.2.2),Pred_or3)
F1_Score(Test3_n$Q6.2.2,Pred_or3)          

(ctable3 <- coef(summary(fit_ordi3)))
p3 <- pnorm(abs(ctable3[, "t value"]), lower.tail = FALSE) * 2
cbind(ctable3, "p value" = p3)
exp(coef(fit_ordi3))
```

With P-values (< 0.05) both the predictors are considered significant. 

Q3.12 - We expect to see 23% (OR = 0.23) decrease in the odds of moving from Unsatisfied to Neutral or Satisfied for one level increase in perceived financial difficulties on a scale of 1-5. There exists an inverse relationship with financial difficulty and satisfaction with wellbeing.

Q6.1.1 - We expect to see 41% (OR = 1.41) increase in the odds of moving from Unsatisfied to Neutral or Satisfied for one level increase in the perceived feeling of being cheerful on a scale of 1-6 (At no Time - All of the Time). Hence, satisfaction with accommodation increases if a student has felt more cheerful over the last couple of weeks.

Because of the proportional odds assumption, similar results apply for predicting Satisfied vs Neutral or Unsatisfied students.
